{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from cyordereddict import OrderedDict\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "from fastai.text import *\n",
    "import html\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import ast\n",
    "import multiprocessing\n",
    "\n",
    "re1 = re.compile(r'  +')\n",
    "chunksize = 100000\n",
    "\n",
    "try:\n",
    "    cpus = multiprocessing.cpu_count()\n",
    "except NotImplementedError:\n",
    "    cpus = 2   # arbitrary default\n",
    "\n",
    "\n",
    "###File Names\n",
    "TEST_NAME = \"Fastai_basic_recipe\"\n",
    "\n",
    "DATASET_FILE_PATH = \"./DataSet/yelp.csv\"\n",
    "WORD2VEC_FILE_PATH = \"./DataSet/FBword2vec/wiki.en.vec\"\n",
    "TRAINING_DATA_PATH = \"./DataSet/trainingData\"\n",
    "EXTENDED_DATASET_PATH = \"../DataSet/review.json\"\n",
    "\n",
    "EMBEDDED_MATRIX_NAME = TEST_NAME+'_embedded_matrix.pkl'\n",
    "TOKENIZED_SENTENCES_NAME = TEST_NAME+ \"_tokenized_sentences.pkl\"\n",
    "WORD_INDEX_NAME = TEST_NAME+ \"_word_index.pkl\"\n",
    "INDEXED_TOKENIZED_SENTENCES_NAME = TEST_NAME + \"_indexed_tokenized_sentences.csv\"\n",
    "\n",
    "LABELS_NAME = 'labels.pkl'\n",
    "\n",
    "DATASET_FILE_PATH = \"../DataSet/yelp.csv/yelp.csv\"\n",
    "TRAINING_DATA_PATH = \"../DataSet/trainingData\"\n",
    "EXTENDED_DATASET_PATH = \"../DataSet/review.json\"\n",
    "\n",
    "BOS  =  'xbos'    # beginning-of-sentence tag \n",
    "FLD = 'xfld'  # data field tag\n",
    "max_vocab = 60000\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast.ai tutorial code\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df['text']\n",
    "    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok\n",
    "\n",
    "def process_text(text):\n",
    "    text = f\"\\n{BOS} {FLD} 1 \" + fixup(text)\n",
    "    return Tokenizer().proc_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = f\"\\n{BOS} {FLD} 1 \" + fixup(text)\n",
    "    return Tokenizer().proc_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading\n",
      "Test split\n",
      "Saved test set\n",
      "Saved train set\n"
     ]
    }
   ],
   "source": [
    "extended_dataset = pd.read_csv(\"Fastai_basic_recipe_tokenized_sentences.csv\")#, chunksize=chunksize)\n",
    "print(\"Finished loading\")\n",
    "train, test = train_test_split(extended_dataset, test_size=0.2)\n",
    "print(\"Test split\")\n",
    "test.to_csv(INDEXED_TOKENIZED_SENTENCES_NAME+\"_test\",encoding='utf-8')\n",
    "print(\"Saved test set\")\n",
    "train.to_csv(INDEXED_TOKENIZED_SENTENCES_NAME+\"_train\",encoding='utf-8')\n",
    "print(\"Saved train set\")\n",
    "sentences = [ast.literal_eval(sentence) for sentence in train['text']]\n",
    "print(\"Transformed strings to list\")\n",
    "Vocab = Counter(word for sentence in sentences for word in sentence)\n",
    "print(\"Made vocab file\")\n",
    "with open('vocab.pkl', 'wb') as outputfile:\n",
    "    pickle.dump(Vocab, outputfile)\n",
    "print(\"Saved vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(INDEXED_TOKENIZED_SENTENCES_NAME+\"_train\", chunksize=chunksize)\n",
    "Vocab = Counter()\n",
    "progress = 0\n",
    "for chunk in train_dataset:\n",
    "    print(progress*chunksize)\n",
    "    sentences = (ast.literal_eval(sentence) for sentence in chunk['text'])\n",
    "    Vocab.update(word for sentence in sentences for word in sentence)\n",
    "    progress+=1\n",
    "print(Vocab)\n",
    "with open('vocab.pkl', 'wb') as outputfile:\n",
    "    pickle.dump(Vocab, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in Vocab.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)\n",
    "with open('itos.pkl', 'wb') as outputfile:\n",
    "    pickle.dump(itos, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2int(text, stoi_dict):\n",
    "    tokens = (word for word in ast.literal_eval(text))\n",
    "    return [stoi_dict[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(INDEXED_TOKENIZED_SENTENCES_NAME+\"_train\", chunksize=chunksize)\n",
    "progress = 0\n",
    "for chunk in train_dataset:\n",
    "    print(progress*chunksize)\n",
    "    chunk.drop(chunk.columns[chunk.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "    chunk['text'] = chunk['text'].apply(token2int, stoi_dict=stoi)\n",
    "    with open(TOKENIZED_SENTENCES_NAME+\"_train_TO_INT\", 'a', encoding=\"utf-8\") as f:\n",
    "        chunk.to_csv(f, header=(progress==0), encoding=\"utf-8\")\n",
    "    progress+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(INDEXED_TOKENIZED_SENTENCES_NAME+\"_test\", chunksize=chunksize)\n",
    "progress = 0\n",
    "for chunk in test_dataset:\n",
    "    print(progress*chunksize)\n",
    "    chunk.drop(chunk.columns[chunk.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "    chunk['text'] = chunk['text'].apply(token2int, stoi_dict=stoi)\n",
    "    with open(TOKENIZED_SENTENCES_NAME+\"_test_TO_INT\", 'a', encoding=\"utf-8\") as f:\n",
    "        chunk.to_csv(f, header=(progress==0), encoding=\"utf-8\")\n",
    "    progress+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WIKITEXT CONVERSION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz,nh,nl = 400,1150,3\n",
    "PRE_PATH = PATH/'models'/'wt103'\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a70d0bfb55b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menc_wgts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_np\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwgts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'0.encoder.weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrow_m\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_wgts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'to_np' is not defined"
     ]
    }
   ],
   "source": [
    "### encoding mean to be used for unknown values\n",
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_itos = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "wiki_stoi = defaultdict(lambda:-1, {v:k for k,v in enumerate(wiki_itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### matching vocabulary\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i,w in enumerate(itos):\n",
    "    r = wiki_stoi[w]\n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### overwrite decoder module in torch\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Language Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_column_number = 9\n",
    "trn_lm = pd.read_csv(TOKENIZED_SENTENCES_NAME+\"_train_TO_INT\", usecols=[sentence_column_number])\n",
    "pool = multiprocessing.Pool(processes=cpus)\n",
    "trn_lm = pool.map(eval, trn_lm['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "lrs = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
