{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from cyordereddict import OrderedDict\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "from fastai.text import *\n",
    "import html\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import ast\n",
    "import multiprocessing\n",
    "\n",
    "re1 = re.compile(r'  +')\n",
    "chunksize = 100000\n",
    "\n",
    "try:\n",
    "    cpus = multiprocessing.cpu_count()\n",
    "except NotImplementedError:\n",
    "    cpus = 2   # arbitrary default\n",
    "\n",
    "pool = multiprocessing.Pool(processes=cpus)\n",
    "    \n",
    "###File Names\n",
    "TEST_NAME = \"Fastai_basic_recipe\"\n",
    "\n",
    "DATASET_FILE_PATH = \"./DataSet/yelp.csv\"\n",
    "WORD2VEC_FILE_PATH = \"./DataSet/FBword2vec/wiki.en.vec\"\n",
    "TRAINING_DATA_PATH = \"./DataSet/trainingData\"\n",
    "EXTENDED_DATASET_PATH = \"../DataSet/review.json\"\n",
    "\n",
    "EMBEDDED_MATRIX_NAME = TEST_NAME+'_embedded_matrix.pkl'\n",
    "TOKENIZED_SENTENCES_NAME = TEST_NAME+ \"_tokenized_sentences.pkl\"\n",
    "WORD_INDEX_NAME = TEST_NAME+ \"_word_index.pkl\"\n",
    "INDEXED_TOKENIZED_SENTENCES_NAME = TEST_NAME + \"_indexed_tokenized_sentences.csv\"\n",
    "\n",
    "PRE_PATH = r\"./models/wt103\"\n",
    "PRE_LM_PATH = r\"./models/fwd_wt103.h5\"\n",
    "\n",
    "LABELS_NAME = 'labels.pkl'\n",
    "\n",
    "DATASET_FILE_PATH = \"../DataSet/yelp.csv/yelp.csv\"\n",
    "TRAINING_DATA_PATH = \"../DataSet/trainingData\"\n",
    "EXTENDED_DATASET_PATH = \"../DataSet/review.json\"\n",
    "\n",
    "BOS  =  'xbos'    # beginning-of-sentence tag \n",
    "FLD = 'xfld'  # data field tag\n",
    "max_vocab = 60000\n",
    "min_freq = 2\n",
    "\n",
    "em_sz,nh,nl = 400,1150,3\n",
    "\n",
    "vs=6002\n",
    "\n",
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast.ai tutorial code\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df['text']\n",
    "    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok\n",
    "\n",
    "def process_text(text):\n",
    "    text = f\"\\n{BOS} {FLD} 1 \" + fixup(text)\n",
    "    return Tokenizer().proc_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = f\"\\n{BOS} {FLD} 1 \" + fixup(text)\n",
    "    return Tokenizer().proc_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_dataset = pd.read_csv(\"Fastai_basic_recipe_tokenized_sentences.csv\")#, chunksize=chunksize)\n",
    "print(\"Finished loading\")\n",
    "train, test = train_test_split(extended_dataset, test_size=0.2)\n",
    "print(\"Test split\")\n",
    "test.to_csv(INDEXED_TOKENIZED_SENTENCES_NAME+\"_test\",encoding='utf-8')\n",
    "print(\"Saved test set\")\n",
    "train.to_csv(INDEXED_TOKENIZED_SENTENCES_NAME+\"_train\",encoding='utf-8')\n",
    "print(\"Saved train set\")\n",
    "sentences = [ast.literal_eval(sentence) for sentence in train['text']]\n",
    "print(\"Transformed strings to list\")\n",
    "Vocab = Counter(word for sentence in sentences for word in sentence)\n",
    "print(\"Made vocab file\")\n",
    "with open('vocab.pkl', 'wb') as outputfile:\n",
    "    pickle.dump(Vocab, outputfile)\n",
    "print(\"Saved vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(INDEXED_TOKENIZED_SENTENCES_NAME+\"_train\", chunksize=chunksize)\n",
    "Vocab = Counter()\n",
    "progress = 0\n",
    "for chunk in train_dataset:\n",
    "    print(progress*chunksize)\n",
    "    sentences = (ast.literal_eval(sentence) for sentence in chunk['text'])\n",
    "    Vocab.update(word for sentence in sentences for word in sentence)\n",
    "    progress+=1\n",
    "print(Vocab)\n",
    "with open('vocab.pkl', 'wb') as outputfile:\n",
    "    pickle.dump(Vocab, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in Vocab.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)\n",
    "with open('itos.pkl', 'wb') as outputfile:\n",
    "    pickle.dump(itos, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2int(text, stoi_dict):\n",
    "    tokens = (word for word in ast.literal_eval(text))\n",
    "    return [stoi_dict[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(INDEXED_TOKENIZED_SENTENCES_NAME+\"_train\", chunksize=chunksize)\n",
    "progress = 0\n",
    "for chunk in train_dataset:\n",
    "    print(progress*chunksize)\n",
    "    chunk.drop(chunk.columns[chunk.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "    chunk['text'] = chunk['text'].apply(token2int, stoi_dict=stoi)\n",
    "    with open(TOKENIZED_SENTENCES_NAME+\"_train_TO_INT\", 'a', encoding=\"utf-8\") as f:\n",
    "        chunk.to_csv(f, header=(progress==0), encoding=\"utf-8\")\n",
    "    progress+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(INDEXED_TOKENIZED_SENTENCES_NAME+\"_test\", chunksize=chunksize)\n",
    "progress = 0\n",
    "for chunk in test_dataset:\n",
    "    print(progress*chunksize)\n",
    "    chunk.drop(chunk.columns[chunk.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "    chunk['text'] = chunk['text'].apply(token2int, stoi_dict=stoi)\n",
    "    with open(TOKENIZED_SENTENCES_NAME+\"_test_TO_INT\", 'a', encoding=\"utf-8\") as f:\n",
    "        chunk.to_csv(f, header=(progress==0), encoding=\"utf-8\")\n",
    "    progress+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WIKITEXT CONVERSION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz,nh,nl = 400,1150,3\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### encoding mean to be used for unknown values\n",
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_itos = pickle.load(open(r'./models/itos_wt103.pkl',\"rb\"))\n",
    "wiki_stoi = defaultdict(lambda:-1, {v:k for k,v in enumerate(wiki_itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### matching vocabulary\n",
    "itos = pickle.load(open(r'itos.pkl',\"rb\"))\n",
    "vs = len(itos)\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i,w in enumerate(itos):\n",
    "    r = wiki_stoi[w]\n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### overwrite decoder module in torch\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Language Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training text\n",
      "train text extracted\n"
     ]
    }
   ],
   "source": [
    "sentence_column_number = 9\n",
    "\n",
    "trn_lm = pd.read_csv(TOKENIZED_SENTENCES_NAME+\"_train_TO_INT\", usecols=[sentence_column_number])\n",
    "print(\"Loaded training text\")\n",
    "trn_lm = np.concatenate(pool.map(eval, trn_lm['text']))\n",
    "print(\"train text extracted\")\n",
    "np.save(\"trn_lm\", trn_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation text\n",
      "validation text extracted\n"
     ]
    }
   ],
   "source": [
    "sentence_column_number = 9\n",
    "val_lm = pd.read_csv(TOKENIZED_SENTENCES_NAME+\"_test_TO_INT\", usecols=[sentence_column_number])\n",
    "print(\"Loaded validation text\")\n",
    "val_lm = np.concatenate(pool.map(eval, val_lm['text']))\n",
    "print(\"validation text extracted\")\n",
    "np.save(\"val_lm\", val_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.load(\"trn_lm.npy\")[:30000]\n",
    "trn_dl = LanguageModelLoader(trn_lm, bs, bptt)\n",
    "val_lm = np.load(\"val_lm.npy\")[:10000]\n",
    "val_dl = LanguageModelLoader(val_lm, bs, bptt)\n",
    "vs=0\n",
    "with open(\"itos.pkl\",\"rb\") as f:\n",
    "    vs = len(pickle.load(f))\n",
    "md = LanguageModelData(\"\", 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "lrs = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12721401b8c408daf3ba616602cb999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                              \n",
      "    0      5.949713   5.84729    0.182389  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([5.84729]), 0.18238866329193115]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
